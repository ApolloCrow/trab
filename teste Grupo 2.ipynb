{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python\n",
    "\n",
    "x = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./teste_var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[45]: 1</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[56]: [MountInfo(mountPoint=&#39;/mnt//adlsg2bvbRO/training&#39;, source=&#39;adl://adlsfordatabricksbvb01.azuredatalakestore.net/training&#39;, encryptionType=&#39;&#39;),\n",
       " MountInfo(mountPoint=&#39;/&#39;, source=&#39;DatabricksRoot&#39;, encryptionType=&#39;&#39;),\n",
       " MountInfo(mountPoint=&#39;/databricks-datasets&#39;, source=&#39;databricks-datasets&#39;, encryptionType=&#39;&#39;),\n",
       " MountInfo(mountPoint=&#39;/databricks-results&#39;, source=&#39;databricks-results&#39;, encryptionType=&#39;&#39;)]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.mounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.unmount(\"/mnt//adlsg2bvbRO/training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "configs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n",
    "           \"dfs.adls.oauth2.client.id\": \"28dd86ad-d24a-4183-9dc7-d42790a1116c\",\n",
    "           \"dfs.adls.oauth2.credential\":\n",
    "           \"Eez/ykW2aVVQrzFKAr1KwB44yHt1xGFpUOjhKFZtpM0=\",\n",
    "           \"dfs.adls.oauth2.refresh.url\":\n",
    "           \"https://login.microsoftonline.com/72f988bf-86f1-41af-91ab2d7cd011db47/oauth2/token\"}\n",
    "\n",
    "#adls_origin = \"adl://adlsfordatabricksbvb01.azuredatalakestore.net/training\"\n",
    "#mount_location = \"/mnt/MyADLS_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mount(\n",
    "  source = \"adl://adlsfordatabricksbvb01.azuredatalakestore.net/training\",\n",
    "  mount_point =  \"/mnt/MyADLS_training\",\n",
    "  extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/mnt/MyADLS_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/MyADLS_training\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-2834658813442741&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>dbutils<span class=\"ansi-blue-fg\">.</span>fs<span class=\"ansi-blue-fg\">.</span>cp<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/mnt/MyADLS_training/_bvb/imdb/ratings/title.basics.tsv&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;/imdb/title.basics.tsv&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/local_disk0/tmp/1583273346117-0/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    306</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    307</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 308</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    309</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    310</span> \n",
       "\n",
       "<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling z:com.databricks.backend.daemon.dbutils.FSUtils.cp.\n",
       ": java.io.FileNotFoundException: /mnt/MyADLS_training/_bvb/imdb/ratings/title.basics.tsv\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:794)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:780)\n",
       "\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:119)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:780)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:780)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:474)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:474)\n",
       "\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:474)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:779)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:197)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$cp$1$$anonfun$apply$3.apply(DBUtilsCore.scala:114)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$cp$1$$anonfun$apply$3.apply(DBUtilsCore.scala:113)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils$.com$databricks$backend$daemon$dbutils$FSUtils$$withFsSafetyCheck(DBUtilsCore.scala:81)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$cp$1.apply(DBUtilsCore.scala:113)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$cp$1.apply(DBUtilsCore.scala:113)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils$.com$databricks$backend$daemon$dbutils$FSUtils$$withFsSafetyCheck(DBUtilsCore.scala:81)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils$.cp(DBUtilsCore.scala:112)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.cp(DBUtilsCore.scala)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:295)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"/mnt/MyADLS_training/_bvb/imdb/ratings/title.basics.tsv\",\"/imdb/title.basics.tsv\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:794)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:780)\n",
       "\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:119)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:780)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:780)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:474)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:474)\n",
       "\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:474)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:779)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:197)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$head$1.apply(DBUtilsCore.scala:192)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$head$1.apply(DBUtilsCore.scala:190)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils$.com$databricks$backend$daemon$dbutils$FSUtils$$withFsSafetyCheck(DBUtilsCore.scala:81)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils$.head(DBUtilsCore.scala:190)\n",
       "\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.head(DbfsUtilsImpl.scala:53)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-625905440410535:1)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-625905440410535:44)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-625905440410535:46)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$read$$iw$$iw$$iw.&lt;init&gt;(command-625905440410535:48)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$read$$iw$$iw.&lt;init&gt;(command-625905440410535:50)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$read$$iw.&lt;init&gt;(command-625905440410535:52)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$read.&lt;init&gt;(command-625905440410535:54)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$read$.&lt;init&gt;(command-625905440410535:58)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$read$.&lt;clinit&gt;(command-625905440410535)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$eval$.$print(&lt;notebook&gt;:6)\n",
       "\tat linef296db9dcbdb4af881ed7d9a0e2b7bc125.$eval.$print(&lt;notebook&gt;)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n",
       "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n",
       "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n",
       "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n",
       "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n",
       "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n",
       "\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:197)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:197)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:197)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:685)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:638)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:197)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:373)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:350)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:350)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n",
       "\tat scala.util.Try$.apply(Try.scala:192)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n",
       "\tat java.lang.Thread.run(Thread.java:748)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs head \"/imdb/title.basics.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_tb = \"/mnt/MyADLS_training/_bvb/imdb/ratings/title.basics.tsv\"\n",
    "\n",
    "movies = spark.read \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option('header', 'true') \\ #Confirmar se é necessário\n",
    "  .option('sep', \"\\t\") \\ #Confirmar se é necessário\n",
    "  .csv(imdb_tb)   \n",
    "\n",
    "display(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_parquet = \"/mnt/MyADLS_training/_bvb/imdb/parquet/films.parquet\"\n",
    "\n",
    "#Confirmar se films já é mesmo a localização final ou é uma \"pasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.coalesce(1).write \\\n",
    "  .option(\"compression\", \"snappy\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .parquet(movies_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_parquet.createOrReplaceTempView(\"movies_parquet_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Path does not exist: dbfs:/mnt/MyADLS_training/_bvb/imdb/parquet/films.parquet;\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:609)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:592)\n",
       "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
       "\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
       "\tat scala.collection.immutable.List.flatMap(List.scala:355)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:592)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:387)\n",
       "\tat org.apache.spark.sql.execution.datasources.CreateTempViewUsing.run(ddl.scala:101)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:72)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:70)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:81)\n",
       "\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:205)\n",
       "\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:205)\n",
       "\tat org.apache.spark.sql.Dataset$$anonfun$56.apply(Dataset.scala:3446)\n",
       "\tat org.apache.spark.sql.Dataset$$anonfun$56.apply(Dataset.scala:3441)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:111)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:240)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:97)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:170)\n",
       "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3441)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)\n",
       "\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:88)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:34)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat scala.collection.immutable.List.map(List.scala:296)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:34)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:373)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:350)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:350)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n",
       "\tat scala.util.Try$.apply(Try.scala:192)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:126)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:373)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:350)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:350)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n",
       "\tat scala.util.Try$.apply(Try.scala:192)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n",
       "\tat java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "CREATE TEMPORARY TABLE filmes\n",
    "  USING parquet\n",
    "    OPTIONS (path \"/mnt/MyADLS_training/_bvb/imdb/parquet/films.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-4069849825612377&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">1</span>\n",
       "<span class=\"ansi-red-fg\">    SELECT *</span>\n",
       "            ^\n",
       "<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> invalid syntax\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "SELECT *\n",
    "  FROM filmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM filmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Table or view not found: filmes; line 1 pos 54\n",
       "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:750)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:695)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:731)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:724)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:77)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:354)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:208)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:352)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:724)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:664)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n",
       "\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:136)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:130)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:102)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:113)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:113)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:80)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:72)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)\n",
       "\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:88)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:34)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat scala.collection.immutable.List.map(List.scala:296)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:34)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:373)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:350)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:350)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n",
       "\tat scala.util.Try$.apply(Try.scala:192)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'filmes' not found in database 'default';\n",
       "\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n",
       "\tat scala.Option.getOrElse(Option.scala:121)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)\n",
       "\tat org.apache.spark.sql.hive.client.PoolingHiveClient.getTable(PoolingHiveClient.scala:42)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:179)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:179)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:144)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:105)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:142)\n",
       "\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:372)\n",
       "\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:358)\n",
       "\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:140)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:178)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:766)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:766)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:144)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:105)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:142)\n",
       "\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:372)\n",
       "\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:358)\n",
       "\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:140)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:765)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:143)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:736)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:747)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:747)\n",
       "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:746)\n",
       "\t... 79 more\n",
       "\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:126)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:373)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:350)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:350)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n",
       "\tat scala.util.Try$.apply(Try.scala:192)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n",
       "\tat java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "SELECT COUNT (DISTINCT StarYear) AS YearDistinct FROM filmes\n",
    "WHERE startyear IS NOT NULL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "teste",
  "notebookId": 836333888804661
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
